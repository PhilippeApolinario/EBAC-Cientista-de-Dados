{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b734a178",
   "metadata": {},
   "source": [
    "# Módulo 24 - Tarefa 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873cd31e",
   "metadata": {},
   "source": [
    "## 1. Cite 5 diferenças entre o AdaBoost e o GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eecdd9",
   "metadata": {},
   "source": [
    "1. Abordagem de construção do modelo: O AdaBoost ajusta os pesos das instâncias dos dados a cada iteração para criar uma série de modelos fracos que são combinados para formar um modelo forte. Por outro lado, o GBM ajusta os resíduos dos modelos anteriores a cada iteração, criando uma série de modelos sucessivos que tentam corrigir os erros dos modelos anteriores.\n",
    "\n",
    "2. Função de perda: O AdaBoost utiliza uma função de perda exponencial que é sensível a outliers. Em contraste, o GBM pode usar várias funções de perda, como a função de perda de desvio absoluto (L1) ou a função de perda de erro quadrático médio (L2), que são menos sensíveis a outliers.\n",
    "\n",
    "3. Número de hiperparâmetros: O AdaBoost tem menos hiperparâmetros para ajustar, geralmente apenas um parâmetro de aprendizado que controla a taxa de aprendizado. O GBM tem mais hiperparâmetros para ajustar, como o número de árvores de decisão, a profundidade máxima da árvore, a taxa de aprendizado e a fração de amostras utilizadas para cada árvore.\n",
    "\n",
    "4. Sensibilidade ao ruído: O AdaBoost é mais sensível ao ruído nos dados de treinamento do que o GBM, pois os erros em instâncias individuais têm um grande efeito no ajuste de pesos. O GBM, por outro lado, pode lidar melhor com dados ruidosos, já que ele se concentra em ajustar os resíduos dos modelos anteriores, em vez de ajustar diretamente os dados de treinamento.\n",
    "\n",
    "5. Velocidade de treinamento: O AdaBoost é mais rápido para treinar do que o GBM, pois ele constrói modelos fracos simples que exigem menos recursos computacionais. Em contraste, o GBM constrói modelos sucessivos mais complexos que exigem mais tempo de processamento para treinar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5831d2c7",
   "metadata": {},
   "source": [
    "## 2. Acesse o link Scikit-learn – GBM, leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "069764e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(\n",
    "    n_estimators=100, \n",
    "    learning_rate=1.0, \n",
    "    max_depth=1, \n",
    "    random_state=0\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bbb94e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46d1f9f",
   "metadata": {},
   "source": [
    "## 3. Cite 5 Hyperparametros importantes no GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dfdf44",
   "metadata": {},
   "source": [
    "1. n_estimators: Número de árvores a serem treinadas.\n",
    "2. learnig_rate: multiplicador da contribuição de cada árvore na obtenção da resposta.\n",
    "3. max_depth: Profundidade máxima de cada árvore.\n",
    "4. max_leaf_nodes: Número máximo de nós de cada árvore.\n",
    "5. loss: Função que será usada para otimizar as árvores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5185e2d6",
   "metadata": {},
   "source": [
    "## 4. Utilize o GridSearch para encontrar os melhores hiperparametros para o conjunto de dados do exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db733f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8602918061035076"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "est = GradientBoostingRegressor()\n",
    "\n",
    "params = {\n",
    "    'n_estimators': np.arange(1, 501, 20),\n",
    "    'learning_rate': np.linspace(0.001, 1.0, 10)\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator = est,\n",
    "    param_grid = params,\n",
    "    scoring = 'r2',\n",
    "    cv = 5\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c1da9",
   "metadata": {},
   "source": [
    "##  5. Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a367a48a",
   "metadata": {},
   "source": [
    "A principal diferença é que no Stochastic GBM é a incorporação da aleatoriedade como parte integral do processo. Em cada iteração uma sub-amostra de base de treino é extraída de maneira aleatória (sem reposição) da base de treino completa. Etapa que não existe no GBM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
